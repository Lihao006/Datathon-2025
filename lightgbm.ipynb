{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153deba7",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import dalex as dx\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from alibi.explainers import Counterfactual\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58139ff",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be91eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "df_train = pd.read_csv('./dataset.csv')\n",
    "X_train_full = df_train.iloc[:,1:].drop('target_variable', axis=1)\n",
    "\n",
    "y_train_full = df_train['target_variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeba74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b187852",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733480de",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b68f5",
   "metadata": {},
   "source": [
    "### Métodos locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# Crear modelo base con LightGBM\n",
    "lgbm = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    random_state=42,\n",
    "    verbosity=-1  # Silenciar logs\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_child_samples': [10, 20]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,        # ⬅️ Solo 5 combinaciones\n",
    "    scoring='f1',\n",
    "    cv=3,            # ⬅️ Solo 3 folds (no 5)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo\n",
    "best_lgbm = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf82bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameters\n",
    "boosting_type \t'gbdt'\n",
    "num_leaves \t31\n",
    "max_depth \t8\n",
    "learning_rate \t0.1\n",
    "n_estimators \t200\n",
    "subsample_for_bin \t200000\n",
    "objective \t'binary'\n",
    "class_weight \tNone\n",
    "min_split_gain \t0.0\n",
    "min_child_weight \t0.001\n",
    "min_child_samples \t10\n",
    "subsample \t1.0\n",
    "subsample_freq \t0\n",
    "colsample_bytree \t1.0\n",
    "reg_alpha \t0.0\n",
    "reg_lambda \t0.0\n",
    "random_state \t42\n",
    "n_jobs \tNone\n",
    "importance_type \t'split'\n",
    "verbosity \t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71427120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "best_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "preds = best_lgbm.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Predecir con el mejor modelo\n",
    "preds_best_lgbm = best_lgbm.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "print(classification_report(y_test, preds_best_lgbm))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm_best = confusion_matrix(y_test, preds_best_lgbm)\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de confusión - Mejor modelo')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicho')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e387d",
   "metadata": {},
   "source": [
    "#### Sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Crear explainer\n",
    "explainer = shap.TreeExplainer(best_lgbm)\n",
    "\n",
    "# Obtener SHAP values\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Si es clasificación binaria, LightGBM también puede devolver una lista\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]  # Tomar la clase positiva (1)\n",
    "\n",
    "# Gráfico global\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca493d",
   "metadata": {},
   "source": [
    "#### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb29fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Asumiendo que ya tienes best_lgbm (tu modelo entrenado con LightGBM)\n",
    "# y X_train, X_test, feature_names definidos\n",
    "\n",
    "# Crear el explainer LIME (igual que con XGBoost)\n",
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values,                   # Datos de entrenamiento\n",
    "    feature_names=feature_names,      # Nombres de las columnas\n",
    "    class_names=['Lost', 'Won'],      # Etiquetas de clase\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Elegir una instancia específica para explicar (ej. índice 0)\n",
    "i = 0\n",
    "exp = explainer_lime.explain_instance(\n",
    "    X_test.iloc[i].values,            # Instancia a explicar\n",
    "    best_lgbm.predict_proba,          # ❗ Aquí usas best_lgbm en lugar de best_bst\n",
    "    num_features=X_test.shape[1]      # Número de variables más influyentes\n",
    ")\n",
    "\n",
    "# Mostrar la explicación\n",
    "exp.save_to_file('lime_explanation.html')  # Guardar como HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa34e8d",
   "metadata": {},
   "source": [
    "### Métodos globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyALE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lista de variables numéricas (excluye categóricas)\n",
    "numeric_features = [\n",
    "    'product_A_sold_in_the_past',\n",
    "    'product_B_sold_in_the_past',\n",
    "    'product_A_recommended',\n",
    "    'product_A',\n",
    "    'product_C',\n",
    "    'product_D',\n",
    "    'cust_hitrate',  \n",
    "    'cust_interactions', \n",
    "    'cust_contracts', \n",
    "    'opp_month', \n",
    "    'opp_old'  \n",
    "]\n",
    "\n",
    "\n",
    "# Generar ALE para cada variable\n",
    "for feature in numeric_features:\n",
    "    try:\n",
    "        ale_eff = PyALE.ale(\n",
    "            X=X_test,\n",
    "            model=best_bst,\n",
    "            feature=[feature],\n",
    "            grid_size=50,\n",
    "            include_CI=False\n",
    "        )\n",
    "        ale_eff.plot()\n",
    "        plt.title(f\"ALE for {feature} (XGBoost)\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando ALE para {feature}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
